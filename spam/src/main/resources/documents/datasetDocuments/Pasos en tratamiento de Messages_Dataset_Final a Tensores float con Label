
--- Quitar label al Full Normalized joined y guardar el orden para agregarlo luego de tokenizar con SecuencePieces.
  El resultado son 2 archivos:
    a) Full Normalized joined sin Label:full_joined_normalized_noduplicates_NoLabel.txt
    b) labels.csv (orden paralelo)

--- Entrenar modelo Tokenización con SentencePiece.
  Con el Full Normalized joined sin Label se obtiene:
  (En carpeta static/modelAndVocabSentencePiece)
        a)tokenizer.model
        b)tokenizer.vocab

---- Tokenizar el mismo archivo que se utilizó para entrenar SentencePiece.
  El resultado es un archivo en carpeta static/mlDatasets:
      * corpus_ids.txt (líneas de enteros separados por espacio)

--- Preparación para DeepNetts
  * Convertir líneas a float[].
  * Agregar padding a todas las líneas.
        - Detectar la longitud máxima de tokens
        - Rellenar con 0f al final de las secuencias más cortas
  * Agregar el label de labels.csv al final de cada línea como último valor
  ➜ Resultado final:
    * dataset_ready.csv → cada línea: float1, float2, ..., floatN, label

